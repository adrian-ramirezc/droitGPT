{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# droitGPT\n",
    "### AI Assistant specialized in French legal codes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The language model\n",
    "### Qwen-1_8B-Chat-Int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/Desktop/MinesParistech2023-2024/NLP/droitGPT/backend/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Salut!\n",
      "Assistant:  Bonjour ! Comment puis-je vous aider aujourd'hui ?\n",
      "Chat history:  [('Salut!', \"Bonjour ! Comment puis-je vous aider aujourd'hui ?\")]\n",
      "\n",
      "User:  Quelle est la capitale du Pérou?\n",
      "Assistant:  La capital du Péninque est conjointement située au sud de la前台, dans le comté de Genève, aux préfectures de Couva et de Moroni.\n",
      "Chat history:  [('Salut!', \"Bonjour ! Comment puis-je vous aider aujourd'hui ?\"), ('Quelle est la capitale du Pérou?', 'La capital du Péninque est conjointement située au sud de la前台, dans le comté de Genève, aux préfectures de Couva et de Moroni.')]\n",
      "\n",
      "User:  Qui est le président du Pérou?\n",
      "Assistant:  Le président du Péninque est actuellement Jean-Paul Bialetti.\n",
      "Chat history:  [('Salut!', \"Bonjour ! Comment puis-je vous aider aujourd'hui ?\"), ('Quelle est la capitale du Pérou?', 'La capital du Péninque est conjointement située au sud de la前台, dans le comté de Genève, aux préfectures de Couva et de Moroni.'), ('Qui est le président du Pérou?', 'Le président du Péninque est actuellement Jean-Paul Bialetti.')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"backend/tokenizer/\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"backend/llm/\", device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "user_q = [\"Salut!\", \"Quelle est la capitale du Pérou?\", \"Qui est le président du Pérou?\"]\n",
    "\n",
    "history_in = None\n",
    "for query in user_q:\n",
    "    response, history_out = model.chat(tokenizer, query, history=history_in)\n",
    "    print(\"User: \", query)\n",
    "    print(\"Assistant: \", response)\n",
    "    print(\"Chat history: \", history_out)\n",
    "    history_in = history_out\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Embeddings Model \n",
    "### Sentence Transformers\n",
    "- Il faut trouver un modèle Sentence Transformers gratuit (donc pas de OpenAI Embeddings ni Ollama Embeddings)\n",
    "- Ce modèle doit être pre-entrainé pour la comparaison semantique\n",
    "- Ce modèle doit comprendre le français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/Desktop/MinesParistech2023-2024/NLP/droitGPT/backend/venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model_id = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database\n",
    "### FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [Document(page_content=\"Le suffrage est direct et universel.\"),\n",
    "        Document(page_content=\"Sont électeurs les Françaises et Français âgés de dix-huit ans accomplis, jouissant de leurs droits civils et politiques et n'étant dans aucun cas d'incapacité prévu par la loi.\"),\n",
    "        Document(page_content=\"Ne doivent pas être inscrits sur la liste électorale, pendant le délai fixé par le jugement, ceux auxquels les tribunaux ont interdit le droit de vote et d'élection, par application des lois qui autorisent cette interdiction.\"),\n",
    "        ]\n",
    "vector = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying the Database\n",
    "On voit que les docs récuperés de la base de données sont un peu similaires au requête (modèle Embeddings n'est pas très grand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_context(docs_and_scores):\n",
    "    for i, (doc, score) in enumerate(docs_and_scores):\n",
    "        print(f\"Top {i+1}\")\n",
    "        print(doc.page_content)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1\n",
      "Le suffrage est direct et universel.\n",
      "\n",
      "Top 2\n",
      "Sont électeurs les Françaises et Français âgés de dix-huit ans accomplis, jouissant de leurs droits civils et politiques et n'étant dans aucun cas d'incapacité prévu par la loi.\n",
      "\n",
      "Top 3\n",
      "Ne doivent pas être inscrits sur la liste électorale, pendant le délai fixé par le jugement, ceux auxquels les tribunaux ont interdit le droit de vote et d'élection, par application des lois qui autorisent cette interdiction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"suffrage universel\"\n",
    "docs_and_scores = vector.similarity_search_with_score(query)\n",
    "print_context(docs_and_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def clean_data(data_folder: str, clean_data_folder: str, files_for_indexing: List[str]):\n",
    "    if not os.path.exists(clean_data_folder):\n",
    "        os.makedirs(clean_data_folder)\n",
    "\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file in files_for_indexing:\n",
    "            file_path = data_folder + \"/\" + file\n",
    "            \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "                pattern1 = re.compile(r\"---.*?---\", re.DOTALL)  # remove title and date\n",
    "                text = re.sub(pattern1, \"\", text)\n",
    "                    \n",
    "                pattern3 = re.compile(\n",
    "                    r\"<div.*</div>\"\n",
    "                )  # remove html content (simple approach, usually tables)\n",
    "                text = re.sub(pattern3, \"\", text)\n",
    "\n",
    "                pattern4 = re.compile(r\"([:;])\\n+\")  # form paragraphs\n",
    "                text = re.sub(pattern4, lambda x: x.group(1) + \" \", text)\n",
    "\n",
    "                add_context_to_article(text=text, clean_data_folder=clean_data_folder, file=file)\n",
    "\n",
    "def add_context_to_article(\n",
    "        clean_data_folder: str,\n",
    "        text: str,\n",
    "        file: str,\n",
    "        init_level=2,\n",
    "        header_name: str = None,\n",
    "    ):\n",
    "        level = init_level\n",
    "        pattern = \"#{level} (.*?)\\n(.*?)(?=\\n#{level} )\"\n",
    "        formatted_pattern = pattern.format(level=\"{\" + str(level) + \"}\")\n",
    "        parts = re.findall(formatted_pattern, text.strip(), re.DOTALL)\n",
    "        if not parts:\n",
    "            if \":\" not in header_name:\n",
    "                header_name = \"\"\n",
    "            else:\n",
    "                header_name = header_name.split(\":\")[-1].strip()\n",
    "\n",
    "            articles = re.findall(r\"\\*\\*(Art\\. .*?)\\*\\*\\n(.*?)(?=\\*\\*)\", text, re.DOTALL)\n",
    "            articles_context = [\n",
    "                (article_name, \"Contexte: \" + header_name + \"\\n\" + article_text)\n",
    "                for article_name, article_text in articles\n",
    "            ]\n",
    "            for article_name, text in articles_context:\n",
    "                text = text.strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                clean_file_path = clean_data_folder + \"/\" + article_name + \"_\" + file\n",
    "                with open(clean_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(text)\n",
    "\n",
    "        for part_name, part_text in parts:\n",
    "            add_context_to_article(\n",
    "                text=part_text,\n",
    "                header_name=part_name,\n",
    "                init_level=level + 1,\n",
    "                file=file,\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce1554bcadd449f061b5777f3a9488b7875d47b72a28be5c12e68c10a50bc98b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
