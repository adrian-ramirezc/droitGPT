{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# droitGPT\n",
    "### AI Assistant specialized in French legal codes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The language model\n",
    "### Qwen-1_8B-Chat-Int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Salut!\n",
      "Assistant:  Bonjour! Comment puis-je vous aider aujourd'hui?\n",
      "Chat history:  [('Salut!', \"Bonjour! Comment puis-je vous aider aujourd'hui?\")]\n",
      "\n",
      "User:  Quelle est la capitale du Pérou?\n",
      "Assistant:  La capitale du Perou est Abénaï.\n",
      "Chat history:  [('Salut!', \"Bonjour! Comment puis-je vous aider aujourd'hui?\"), ('Quelle est la capitale du Pérou?', 'La capitale du Perou est Abénaï.')]\n",
      "\n",
      "User:  Qui est le président du Pérou?\n",
      "Assistant:  Le président du Perou actuel est Jamila Mahmoudine Boulidi.\n",
      "Chat history:  [('Salut!', \"Bonjour! Comment puis-je vous aider aujourd'hui?\"), ('Quelle est la capitale du Pérou?', 'La capitale du Perou est Abénaï.'), ('Qui est le président du Pérou?', 'Le président du Perou actuel est Jamila Mahmoudine Boulidi.')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"backend/tokenizer/\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"backend/llm/\", device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "user_q = [\"Salut!\", \"Quelle est la capitale du Pérou?\", \"Qui est le président du Pérou?\"]\n",
    "\n",
    "history_in = None\n",
    "for query in user_q:\n",
    "    response, history_out = model.chat(tokenizer, query, history=history_in)\n",
    "    print(\"User: \", query)\n",
    "    print(\"Assistant: \", response)\n",
    "    print(\"Chat history: \", history_out)\n",
    "    history_in = history_out\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Embeddings Model \n",
    "### Sentence Transformers\n",
    "- Il faut trouver un modèle Sentence Transformers gratuit (donc pas de OpenAI Embeddings ni Ollama Embeddings)\n",
    "- Ce modèle doit être pre-entrainé pour la comparaison semantique\n",
    "- Ce modèle doit comprendre le français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/Desktop/MinesParistech2023-2024/NLP/droitGPT/backend/venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model_id = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database\n",
    "### FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "docs = [Document(page_content=\"Le suffrage est direct et universel.\"),\n",
    "        Document(page_content=\"Sont électeurs les Françaises et Français âgés de dix-huit ans accomplis, jouissant de leurs droits civils et politiques et n'étant dans aucun cas d'incapacité prévu par la loi.\"),\n",
    "        Document(page_content=\"Ne doivent pas être inscrits sur la liste électorale, pendant le délai fixé par le jugement, ceux auxquels les tribunaux ont interdit le droit de vote et d'élection, par application des lois qui autorisent cette interdiction.\"),\n",
    "        ]\n",
    "vector = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying the Database\n",
    "On voit que les docs récuperés de la base de données sont un peu similaires au requête (modèle Embeddings n'est pas très grand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_context(docs_and_scores):\n",
    "    for i, (doc, score) in enumerate(docs_and_scores):\n",
    "        print(f\"Top {i+1}\")\n",
    "        print(doc.page_content)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1\n",
      "Ne doivent pas être inscrits sur la liste électorale, pendant le délai fixé par le jugement, ceux auxquels les tribunaux ont interdit le droit de vote et d'élection, par application des lois qui autorisent cette interdiction.\n",
      "\n",
      "Top 2\n",
      "Sont électeurs les Françaises et Français âgés de dix-huit ans accomplis, jouissant de leurs droits civils et politiques et n'étant dans aucun cas d'incapacité prévu par la loi.\n",
      "\n",
      "Top 3\n",
      "Le suffrage est direct et universel.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Qui sont les electeurs?\"\n",
    "docs_and_scores = vector.similarity_search_with_score(query)\n",
    "print_context(docs_and_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "\n",
    "clean_data_folder = \"new_clean_data\"\n",
    "data_folder = \"backend/data\"\n",
    "files_for_indexing = [\"electoral.md\"]\n",
    "\n",
    "# TODO\n",
    "# remove title and date\n",
    "# remove html content\n",
    "\n",
    "# divide by markdown title and get title name\n",
    "# do this recursive until there is no title\n",
    "# create file name based on titles \n",
    "# save file name\n",
    "\n",
    "def clean_data():\n",
    "    if not os.path.exists(clean_data_folder):\n",
    "        os.makedirs(clean_data_folder)\n",
    "\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file in files_for_indexing:\n",
    "            file_path = data_folder + \"/\" + file\n",
    "            \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "                pattern1 = re.compile(r\"---.*?---\", re.DOTALL)  # remove title and date\n",
    "                text = re.sub(pattern1, \"\", text)\n",
    "\n",
    "                pattern3 = re.compile(\n",
    "                    r\"<div.*</div>\"\n",
    "                )  # remove html content (simple approach, usually tables)\n",
    "                text = re.sub(pattern3, \"\", text)\n",
    "\n",
    "                pattern4 = re.compile(r\"([:;])\\n+\")  # form paragraphs\n",
    "                text = re.sub(pattern4, lambda x: x.group(1) + \" \", text)\n",
    "\n",
    "                articles = re.findall(r\"\\*\\*(Art\\. [^\\*]+)\\*\\*\\n([^\\*]+)\", text.strip())\n",
    "                for article_name, text in articles:\n",
    "                    if \"tableau\" not in article_name:\n",
    "                        clean_file_path = clean_data_folder + \"/\" + article_name + \"_\" + file\n",
    "                        with open(clean_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(text.strip())\n",
    "clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown_it\n",
    "\n",
    "def parse_markdown_to_dict(markdown_content):\n",
    "    md = markdown_it.MarkdownIt()\n",
    "\n",
    "    tokens = md.parse(markdown_content, {})\n",
    "    \n",
    "    result_dict = {}\n",
    "    current_key = None\n",
    "    current_value = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.type == 'heading_open':\n",
    "            current_key = token['tag']\n",
    "            current_value = []\n",
    "        elif token.type == 'inline':\n",
    "            current_value.append(token['content'])\n",
    "        elif token.type == 'heading_close':\n",
    "            result_dict[current_key] = ''.join(current_value).strip()\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown_it\n",
    "\n",
    "def parse_markdown_to_nested_dict(markdown_content):\n",
    "    md = markdown_it.MarkdownIt()\n",
    "\n",
    "    tokens = md.parse(markdown_content, {})\n",
    "\n",
    "    def process_tokens(tokens):\n",
    "        result_dict = {}\n",
    "        current_key = None\n",
    "        current_value = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token.type == 'heading_open':\n",
    "                heading_level = int(token.tag[1])\n",
    "                current_key = token.content\n",
    "                current_value = process_tokens(tokens)\n",
    "                result_dict[heading_level] = {current_key: current_value}\n",
    "            elif token.type == \"inline\":\n",
    "                current_value.append(token.content)\n",
    "            elif token.type == 'heading_close':\n",
    "                return ''.join(current_value).strip()\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    return process_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m markdown_file:\n\u001b[1;32m      6\u001b[0m         markdown_content \u001b[39m=\u001b[39m markdown_file\u001b[39m.\u001b[39mread()\n\u001b[0;32m----> 7\u001b[0m         parsed_dict \u001b[39m=\u001b[39m parse_markdown_to_nested_dict(markdown_content)\n\u001b[1;32m      8\u001b[0m         \u001b[39mprint\u001b[39m(parsed_dict)\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mparse_markdown_to_nested_dict\u001b[0;34m(markdown_content)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(current_value)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m result_dict\n\u001b[0;32m---> 26\u001b[0m \u001b[39mreturn\u001b[39;00m process_tokens(tokens)\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mparse_markdown_to_nested_dict.<locals>.process_tokens\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     15\u001b[0m     heading_level \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(token\u001b[39m.\u001b[39mtag[\u001b[39m1\u001b[39m])\n\u001b[1;32m     16\u001b[0m     current_key \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39mcontent\n\u001b[0;32m---> 17\u001b[0m     current_value \u001b[39m=\u001b[39m process_tokens(tokens)\n\u001b[1;32m     18\u001b[0m     result_dict[heading_level] \u001b[39m=\u001b[39m {current_key: current_value}\n\u001b[1;32m     19\u001b[0m \u001b[39melif\u001b[39;00m token\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minline\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mparse_markdown_to_nested_dict.<locals>.process_tokens\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     15\u001b[0m     heading_level \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(token\u001b[39m.\u001b[39mtag[\u001b[39m1\u001b[39m])\n\u001b[1;32m     16\u001b[0m     current_key \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39mcontent\n\u001b[0;32m---> 17\u001b[0m     current_value \u001b[39m=\u001b[39m process_tokens(tokens)\n\u001b[1;32m     18\u001b[0m     result_dict[heading_level] \u001b[39m=\u001b[39m {current_key: current_value}\n\u001b[1;32m     19\u001b[0m \u001b[39melif\u001b[39;00m token\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minline\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "    \u001b[0;31m[... skipping similar frames: parse_markdown_to_nested_dict.<locals>.process_tokens at line 17 (2959 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mparse_markdown_to_nested_dict.<locals>.process_tokens\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     15\u001b[0m     heading_level \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(token\u001b[39m.\u001b[39mtag[\u001b[39m1\u001b[39m])\n\u001b[1;32m     16\u001b[0m     current_key \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39mcontent\n\u001b[0;32m---> 17\u001b[0m     current_value \u001b[39m=\u001b[39m process_tokens(tokens)\n\u001b[1;32m     18\u001b[0m     result_dict[heading_level] \u001b[39m=\u001b[39m {current_key: current_value}\n\u001b[1;32m     19\u001b[0m \u001b[39melif\u001b[39;00m token\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minline\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mparse_markdown_to_nested_dict.<locals>.process_tokens\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      4\u001b[0m md \u001b[39m=\u001b[39m markdown_it\u001b[39m.\u001b[39mMarkdownIt()\n\u001b[1;32m      6\u001b[0m tokens \u001b[39m=\u001b[39m md\u001b[39m.\u001b[39mparse(markdown_content, {})\n\u001b[0;32m----> 8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_tokens\u001b[39m(tokens):\n\u001b[1;32m      9\u001b[0m     result_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m     10\u001b[0m     current_key \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "file = \"electoral.md\"\n",
    "file_path = data_folder + \"/\" + file\n",
    "clean_file_path = clean_data_folder + \"/clean_\" + file\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as markdown_file:\n",
    "        markdown_content = markdown_file.read()\n",
    "        parsed_dict = parse_markdown_to_nested_dict(markdown_content)\n",
    "        print(parsed_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce1554bcadd449f061b5777f3a9488b7875d47b72a28be5c12e68c10a50bc98b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
